Great motive ğŸ‘ â€” this is a very common and good architecture.
Let me walk you through it step-by-step, end-to-end, using Podman + Redis + Python + FastAPI, and explain why each piece exists.


---

Big Picture (What you are building)

Python Producer (every 1 min)
        |
        v
     Redis  <---- FastAPI (Redis cache)
        |
        v
     Clients

A Python script pushes fresh data into Redis every 1 minute

FastAPI reads from Redis

Redis acts as:

Primary store for latest data

Cache so repeated requests donâ€™t recompute data




---

1ï¸âƒ£ Redis (already running)

Redis container (as mycompute):

podman ps

You should see Redis running on:

localhost:6379


---

2ï¸âƒ£ Install Python dependencies (host machine)

As user mycompute:

python3 -m venv venv
source venv/bin/activate

pip install redis fastapi uvicorn


---

3ï¸âƒ£ Python script: push data every 1 minute

producer.py

import time
import redis
import json
from datetime import datetime

r = redis.Redis(host="localhost", port=6379, decode_responses=True)

while True:
    data = {
        "timestamp": datetime.utcnow().isoformat(),
        "value": round(time.time() % 100, 2)
    }

    r.set("latest_data", json.dumps(data))
    print("Pushed:", data)

    time.sleep(60)

Run it:

python producer.py

ğŸ“Œ This:

Updates Redis every 60 seconds

Always keeps latest_data fresh



---

4ï¸âƒ£ Verify data in Redis

podman exec -it redis redis-cli

GET latest_data

You should see JSON data.


---

5ï¸âƒ£ FastAPI app: pull data from Redis (cached)

main.py

from fastapi import FastAPI
import redis
import json

app = FastAPI()

r = redis.Redis(host="localhost", port=6379, decode_responses=True)

@app.get("/data")
def get_data():
    cached = r.get("latest_data")

    if cached:
        return {
            "source": "redis-cache",
            "data": json.loads(cached)
        }

    return {"error": "No data found"}


---

6ï¸âƒ£ Run FastAPI

uvicorn main:app --host 0.0.0.0 --port 8000

Open browser or curl:

curl http://localhost:8000/data

Youâ€™ll get:

{
  "source": "redis-cache",
  "data": {
    "timestamp": "2025-12-19T12:00:00",
    "value": 42.3
  }
}


---

7ï¸âƒ£ Why this is â€œcachedâ€

Producer updates Redis once per minute

FastAPI:

Does not compute anything

Simply fetches cached value


Multiple API requests â†’ same Redis data

No recomputation or DB hits



---

8ï¸âƒ£ Optional: Add TTL (auto-expire data)

If you want data to expire automatically:

In producer:

r.setex("latest_data", 70, json.dumps(data))

This:

Expires key after 70 seconds

Prevents stale data



---

9ï¸âƒ£ Optional: Use Redis hash instead of string

r.hset("latest_data", mapping=data)

Retrieve:

r.hgetall("latest_data")


---

10ï¸âƒ£ Production-style improvement (later)

When youâ€™re ready:

Run FastAPI in a container

Connect via Podman network

Add Redis connection pool

Add background tasks instead of while loop

Use Celery / APScheduler



---

Mental Model (Important)

Redis here is NOT just cache, it is:

A shared memory store

Updated by one process

Read by many processes

Fast, in-memory, reliable



---

âœ… You are building the RIGHT thing

This exact pattern is used in:

Real-time dashboards

Metrics systems

IoT ingestion

Rate-limited APIs

High-traffic backends



---

If you want next, I can help you:

Run FastAPI in Podman

Use Redis pub/sub instead of polling

Add authentication

Add background tasks in FastAPI

Structure project for production


Just tell me what you want next ğŸ‘Œ
